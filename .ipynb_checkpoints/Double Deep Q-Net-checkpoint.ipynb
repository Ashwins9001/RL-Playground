{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions, discrete=False):\n",
    "        self.mem_size = max_size\n",
    "        \n",
    "        #rather than deque, use set np arrays, track index last saved\n",
    "        #store tuple of reward, q-vals, next state for experience replay\n",
    "        self.mem_counter = 0 \n",
    "        self.discrete = discrete\n",
    "        \n",
    "        #allocate table equal to rows of mem entries with cols of env observations (e.g. for lunar lander-> 8 possibile results)\n",
    "        self.state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        dtype = np.int8 if self.discrete else np.float32 #for continuous actions, will be decimals, discrete space int w/ 8 possibilities\n",
    "        \n",
    "        #set dtype to index np array for experience replay, table will store either int or decimals depending on dtype \n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions), dtype=dtype) #possible actions for lunar lander-> 4\n",
    "        self.new_state_memory = np.zeros((self.mem_size, input_shape))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32) #sampling transitions for eps, future reward at terminal state is zero, must accomodate and store incase\n",
    "        \n",
    "    #add transitions to mem    \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size #ensure mem overwritten when mem_size surpassed \n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        if self.discrete:\n",
    "            #retrieve num actions from cols of action_mem if discrete space\n",
    "            actions = np.zeros(self.action_memory.shape[1])\n",
    "            #provide one-hot encoding for selected action, ex: [0, 0, 0, 1, 0, 0, 0, 0] -> at state x, agent takes actions[3] = 1, goes to state_ y\n",
    "            actions[action] = 1.0\n",
    "            #store entire arr of actions at each index in mem\n",
    "            self.action_memory[index] = actions\n",
    "        else:\n",
    "            self.action_memory[action] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = 1 - int(done)\n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    #define sample size for mem \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
