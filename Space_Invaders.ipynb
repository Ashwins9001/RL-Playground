{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame size:  Box(210, 160, 3)\n",
      "Actions available:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "print(\"Frame size: \", env.observation_space)\n",
    "print(\"Actions available: \", env.action_space.n)\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing params\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4) #clear stack \n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    gray = rgb2gray(frame)\n",
    "    cropped_frame = gray[8:-12, 4:-12]\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110, 84])\n",
    "    return preprocessed_frame\n",
    "\n",
    "#Skip four frames each timestep and stack those frames into queue to provide network sense of position, velocity, acceleration\n",
    "#Appending frame to deque removes oldest frame on stack; formulate state\n",
    "def stack_frames(stacked_frames, state, is_new_eps):\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_eps: #on new eps\n",
    "        stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4) #clear stack \n",
    "        stacked_frames.append(frame) #init stack on eps\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2) #joins frames\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model params defining MDP\n",
    "state_size = [110, 84, 4] #provide stack 4 frames each 110 x 84; each action repeatedly performed for four frames of stack\n",
    "action_size = env.action_space.n #8 possible actions\n",
    "learning_rate = 0.00025 #alpha\n",
    "\n",
    "#Training params defining how DQN will learn\n",
    "total_eps = 50\n",
    "max_steps = 50000\n",
    "batch_size = 64\n",
    "\n",
    "#Exploration params for epsilon-greedy action selection\n",
    "explore_start = 1.0 #max exploration prob\n",
    "explore_stop = 0.01 #min exploration prob\n",
    "decay_rate = 0.00001 #gamma param extremely low thus agent will value actions taken long ago\n",
    "\n",
    "#Q-learning params\n",
    "pretrain_length = batch_size #experiences in mem at init\n",
    "memory_size = 1000000 #experiences stored in mem to improve convergence time to optimal state-action values\n",
    "\n",
    "\n",
    "training = False\n",
    "eps_render = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to define architecture of DNN, no training/learning/init done \n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions_\")\n",
    "            \n",
    "            #sample episilon-greedy action step into env, observe next states, rewards and provide to DNN upon init\n",
    "            #purpose of stepping through env is to gain exp and iteratively train DNN by minimizing loss \n",
    "            #receive set of q-values per actions from DNN, select max and set as target_Q\n",
    "            #store experience (reward, action, state, next_state) at time step in replay memory\n",
    "            #using experiences, compute target Q-value which is desirable distribution DNN must approximate\n",
    "            #iterate to following state\n",
    "            #load up experiences and provide as vector of inputs to DNN and compute prediction per each experience\n",
    "            #apply loss function that computes error between DNN output and optimal distribution from stored experiences in memory\n",
    "            #backpropagate computations to tune weights & repeat; DNN is learning \n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\") \n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_, \n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out, \n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out, \n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units=self.action_size,\n",
    "                                          activation=None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_)) #predicted Q-value computed by DNN\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-9644a204c165>:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply experience replay to ensure agent correctly behaves to previously trained envs \n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arrange(buffer_size),\n",
    "                                 size = batch_size,\n",
    "                                 replace = False)\n",
    "        return [self.buffer[i] for i in index]\n",
    "            \n",
    "#Populate memory by taking rand actions, storing experience (tuple of: state, action, reward, next_state)\n",
    "memory = Memory(max_size = memory_size)\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0: #first step\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    choice = random.randint(1, len(possible_actions)) - 1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer; used to visualize graphical struc of NN \n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use epsilon-greedy action selection\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    explore_prob = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    #Explore random action\n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "    #Choose most valuable greedy action using DQN\n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train DQN using stored experiences and estimated network output at each time-step until loss is minimal & func approx optimized\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
