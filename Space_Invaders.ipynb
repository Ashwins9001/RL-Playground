{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame size:  Box(210, 160, 3)\n",
      "Actions available:  8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "print(\"Frame size: \", env.observation_space)\n",
    "print(\"Actions available: \", env.action_space.n)\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing params\n",
    "stack_size = 4\n",
    "stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4) #clear stack \n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    gray = rgb2gray(frame)\n",
    "    cropped_frame = gray[8:-12, 4:-12]\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110, 84])\n",
    "    return preprocessed_frame\n",
    "\n",
    "#Skip four frames each timestep and stack those frames into queue to provide network sense of position, velocity, acceleration\n",
    "#Appending frame to deque removes oldest frame on stack; formulate state\n",
    "def stack_frames(stacked_frames, state, is_new_eps):\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_eps: #on new eps\n",
    "        stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen=4) #clear stack \n",
    "        stacked_frames.append(frame) #init stack on eps\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis=2) #joins frames\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model params defining MDP\n",
    "state_size = [110, 84, 4] #provide stack 4 frames each 110 x 84; each action repeatedly performed for four frames of stack\n",
    "action_size = env.action_space.n #8 possible actions\n",
    "learning_rate = 0.00025 #alpha\n",
    "\n",
    "#Training params defining how DQN will learn\n",
    "total_episodes = 50\n",
    "max_steps = 50000\n",
    "batch_size = 64\n",
    "\n",
    "#Exploration params for epsilon-greedy action selection\n",
    "explore_start = 1.0 #max exploration prob\n",
    "explore_stop = 0.01 #min exploration prob\n",
    "decay_rate = 0.00001 #gamma param extremely low thus agent will value actions taken long ago\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "#Q-learning params\n",
    "pretrain_length = batch_size #experiences in mem at init\n",
    "memory_size = 1000000 #experiences stored in mem to improve convergence time to optimal state-action values\n",
    "\n",
    "\n",
    "training = True\n",
    "episode_render = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class to define architecture of DNN, no training/learning/init done \n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions_\")\n",
    "            \n",
    "            #sample episilon-greedy action step into env, observe next states, rewards and provide to DNN upon init\n",
    "            #purpose of stepping through env is to gain exp and iteratively train DNN by minimizing loss \n",
    "            #receive set of q-values per actions from DNN, select max and set as target_Q\n",
    "            #store experience (reward, action, state, next_state) at time step in replay memory\n",
    "            #using experiences, compute target Q-value which is desirable distribution DNN must approximate\n",
    "            #iterate to following state\n",
    "            #load up experiences and provide as vector of inputs to DNN and compute prediction per each experience\n",
    "            #apply loss function that computes error between DNN output and optimal distribution from stored experiences in memory\n",
    "            #backpropagate computations to tune weights & repeat; DNN is learning \n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\") \n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_, \n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out, \n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out, \n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units=self.action_size,\n",
    "                                          activation=None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_)) #predicted Q-value computed by DNN\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-9644a204c165>:29: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Owner\\Anaconda3\\envs\\pole-balance\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-9644a204c165>:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply experience replay to ensure agent correctly behaves to previously trained envs \n",
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        #start from beg, increment by 1 & end of range at buffer_size; sample num from buffer & apply to batch\n",
    "        #buffer reps all experiences collected upto max_size, sample from it and repeat op over batch_size\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size = batch_size,\n",
    "                                 replace = False)\n",
    "        return [self.buffer[i] for i in index]\n",
    "            \n",
    "#Populate memory by taking rand actions, storing experience (tuple of: state, action, reward, next_state)\n",
    "memory = Memory(max_size = memory_size)\n",
    "for i in range(pretrain_length):\n",
    "    if i == 0: #first step\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    choice = random.randint(1, len(possible_actions)) - 1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer; used to visualize graphical struc of NN \n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use epsilon-greedy action selection\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    #Explore random action\n",
    "    if(explore_probability > exp_tradeoff):\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "    #Choose most valuable greedy action using DQN\n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train DQN using stored experiences and estimated network output at each time-step until loss is minimal & func approx optimized\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        decay_step = 0\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0\n",
    "            episode_rewards = []\n",
    "            state = env.reset()\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                decay_step += 1\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "                next_state, reward, done, information = env.step(action)\n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((110, 84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    step = max_steps\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    print('Episode: {}'.format(episode),'Total reward: {}'.format(total_reward),'Explore P: {:.4f}'.format(explore_probability), 'Training Loss {:.4f}'.format(loss))\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                else:\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    state = next_state\n",
    "                    \n",
    "                #Retrieve information from memory \n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch])\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                #Obtain expected Q-vals per all following states to compute Q-val for current state: target_Q\n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                #Set Q_target = r if the episode ends at next-state, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                #Collect Q-vals of current states for entire batch of actions        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                #Compute loss by comparing calculated Q_target val against predicted Q-val found via DQN\n",
    "                #Feed vals to placeholders of tf graph for computation\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer], feed_dict={DQNetwork.inputs_:states_mb, DQNetwork.target_Q: targets_mb, DQNetwork.actions_: actions_mb})\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_:states_mb, DQNetwork.target_Q: targets_mb, DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\") \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'saver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-26af19b1df36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtotal_test_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./models/model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'saver' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(1):\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "        \n",
    "        while True:\n",
    "            # Reshape the state\n",
    "            state = state.reshape((1, *state_size))\n",
    "\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state})\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                print (\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "                \n",
    "                \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
